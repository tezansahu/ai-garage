{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Phi-4 for Medical Reasoning: From Traditional to Reasoning SLM\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to transform Microsoft's Phi-4, a traditional small language model (SLM), into a reasoning-capable model for medical applications. We'll use the `unsloth` library to efficiently fine-tune the model on the `o1-medical-reasoning` dataset, teaching it to \"think\" through medical problems step-by-step before providing answers.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Small Language Models (SLMs)** and their advantages\n",
    "2. **Reasoning vs Non-Reasoning Models** and their differences  \n",
    "3. **Model Quantization** for memory-efficient training\n",
    "4. **Parameter Efficient Fine-Tuning (PEFT)** with LoRA\n",
    "5. **Supervised Fine-Tuning (SFT)** techniques\n",
    "6. **Model deployment** with interactive interfaces\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Basic Python programming knowledge\n",
    "- Understanding of machine learning concepts\n",
    "- Access to GPU with at least 16GB VRAM (recommended)\n",
    "\n",
    "---\n",
    "\n",
    "![](./assets/finetuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Small Language Models (SLMs)\n",
    "\n",
    "### What are Small Language Models?\n",
    "\n",
    "Small Language Models (SLMs) are compact versions of large language models, typically containing **1-15 billion parameters** compared to the **100+ billion parameters** found in models like GPT-4 or Claude. Despite their smaller size, they offer several advantages:\n",
    "\n",
    "**Key Advantages of SLMs:**\n",
    "- ğŸš€ **Speed**: Faster inference times due to fewer parameters\n",
    "- ğŸ’¾ **Memory Efficiency**: Lower VRAM requirements (can run on consumer GPUs)\n",
    "- âš¡ **Cost-Effective**: Cheaper to train and deploy\n",
    "- ğŸ¯ **Task-Specific**: Can be highly optimized for specific domains\n",
    "- ğŸ  **On-Device Deployment**: Can run locally without internet connectivity\n",
    "\n",
    "![](./assets/llm-vs-slm.png)\n",
    "\n",
    "### Microsoft Phi-4: Our Base Model\n",
    "\n",
    "**Phi-4** is Microsoft's latest small language model with:\n",
    "- **14 billion parameters**\n",
    "- **BF16 precision** (~25GB when fully loaded)\n",
    "- **High performance** on reasoning benchmarks\n",
    "- **Optimized architecture** for efficiency\n",
    "\n",
    "![](./assets/phi-4.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reasoning vs Non-Reasoning Models\n",
    "\n",
    "### Traditional (Non-Reasoning) Models\n",
    "\n",
    "Most language models, including the base Phi-4, are **non-reasoning models**. They:\n",
    "- Generate responses **immediately** based on patterns learned during training\n",
    "- **Don't show their work** or explain their thought process\n",
    "- Can make mistakes due to **lack of deliberation**\n",
    "- Are **fast** but may lack depth in complex problem-solving\n",
    "\n",
    "### Reasoning Models (Like OpenAI's o1)\n",
    "\n",
    "Reasoning models introduce a **\"thinking\" phase** before responding:\n",
    "- **Chain-of-Thought**: Step-by-step problem breakdown\n",
    "- **Self-reflection**: Ability to reconsider and correct reasoning\n",
    "- **Explicit reasoning**: Show intermediate steps\n",
    "- **Higher accuracy** on complex problems, especially in STEM and medical domains\n",
    "\n",
    "![](./assets/regular-vs-reasoning.png)\n",
    "\n",
    "### The Medical Reasoning Challenge\n",
    "\n",
    "Medical diagnosis requires:\n",
    "1. **Symptom analysis**: Understanding patient presentation\n",
    "2. **Differential diagnosis**: Considering multiple possibilities  \n",
    "3. **Evidence weighing**: Balancing different clinical indicators\n",
    "4. **Systematic thinking**: Following medical reasoning protocols\n",
    "\n",
    "\n",
    "### Our Goal: Transform Phi-4\n",
    "\n",
    "We'll teach Phi-4 to:\n",
    "- **Think step-by-step** using `<think>` tags\n",
    "- **Show medical reasoning** process explicitly\n",
    "- **Provide final answers** after deliberation\n",
    "- **Handle complex medical scenarios** accurately\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Environment\n",
    "\n",
    "### What is Unsloth?\n",
    "\n",
    "[**Unsloth**](https://docs.unsloth.ai/) is an open-source library that makes fine-tuning large language models **2x faster** and uses **30% less memory**. \n",
    "\n",
    "### Key features:\n",
    "\n",
    "- ğŸš€ **Speed optimization**: Optimized kernels for training\n",
    "- ğŸ’¾ **Memory efficiency**: Advanced memory management\n",
    "- ğŸ”§ **Easy-to-use**: Simplified APIs for complex operations\n",
    "- ğŸ¯ **PEFT support**: Built-in LoRA and other efficient fine-tuning methods\n",
    "- ğŸ“Š **Multi-format support**: Works with various model architectures\n",
    "\n",
    "### Installation\n",
    "\n",
    "Let's install the Unsloth library to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:12.788122Z",
     "iopub.status.busy": "2025-08-25T19:37:12.787561Z",
     "iopub.status.idle": "2025-08-25T19:41:38.460315Z",
     "shell.execute_reply": "2025-08-25T19:41:38.459494Z",
     "shell.execute_reply.started": "2025-08-25T19:37:12.788098Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Using cached unsloth-2025.8.9-py3-none-any.whl.metadata (52 kB)\n",
      "Collecting unsloth_zoo>=2025.8.8 (from unsloth)\n",
      "  Using cached unsloth_zoo-2025.8.8-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Using cached xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting bitsandbytes (from unsloth)\n",
      "  Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (25.0)\n",
      "Collecting tyro (from unsloth)\n",
      "  Using cached tyro-0.9.28-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.52.4)\n",
      "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.6.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.8.1)\n",
      "Collecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n",
      "  Using cached trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n",
      "Collecting huggingface_hub>=0.34.0 (from unsloth)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.34.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (0.21.2)\n",
      "Collecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 (from unsloth)\n",
      "  Using cached transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: torchao in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.8.8->unsloth) (0.10.0)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.8.8->unsloth)\n",
      "  Using cached cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.8.8->unsloth) (11.2.1)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.8.8->unsloth)\n",
      "  Using cached msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting torch>=2.4.0 (from unsloth)\n",
      "  Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.2.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from unsloth)\n",
      "  Using cached torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (14.0.0)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Using cached shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (3.12.13)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2025.6.15)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.20.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.17.0)\n",
      "Using cached unsloth-2025.8.9-py3-none-any.whl (311 kB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached trl-0.21.0-py3-none-any.whl (511 kB)\n",
      "Using cached transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "Using cached unsloth_zoo-2025.8.8-py3-none-any.whl (184 kB)\n",
      "Using cached xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
      "Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
      "Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
      "Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m951.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.28-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m245.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface_hub, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, transformers, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.33.1\n",
      "    Uninstalling huggingface-hub-0.33.1:\n",
      "      Successfully uninstalled huggingface-hub-0.33.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cu124\n",
      "    Uninstalling torchvision-0.21.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.47.0 cut_cross_entropy-25.1.1 fsspec-2025.3.0 huggingface_hub-0.34.4 msgspec-0.19.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 shtab-1.7.2 sympy-1.14.0 torch-2.8.0 torchvision-0.23.0 transformers-4.55.4 triton-3.4.0 trl-0.21.0 tyro-0.9.28 unsloth-2025.8.9 unsloth_zoo-2025.8.8 xformers-0.0.32.post2\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install unsloth\n",
    "!pip install transformers==4.55.4   # Installing this version of transformers as it is compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Quantization and Loading\n",
    "\n",
    "### Understanding Quantization\n",
    "\n",
    "**Quantization** reduces model memory usage by using lower-precision number formats:\n",
    "\n",
    "- **BF16** (16-bit): Original Phi-4 format (~25GB)\n",
    "- **INT4** (4-bit): ~75% memory reduction (~14GB for Phi-4)\n",
    "\n",
    "**4-bit Quantization Benefits:**\n",
    "- ğŸ¯ **Memory Efficient**: Fits larger models on consumer GPUs\n",
    "- âš¡ **Faster Loading**: Reduced data transfer\n",
    "- ğŸª **Maintained Quality**: Minimal performance degradation with modern techniques\n",
    "- ğŸ—ï¸ **Training Compatible**: Can fine-tune quantized models\n",
    "\n",
    "\n",
    "### Loading Phi-4 with Unsloth\n",
    "\n",
    "We'll load the Phi-4 model with the following configuration:\n",
    "- **4-bit quantization** for memory efficiency\n",
    "- **2048 token context** length  \n",
    "- **RoPE scaling** support for longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:37:05.416136Z",
     "iopub.status.busy": "2025-08-25T19:37:05.415549Z",
     "iopub.status.idle": "2025-08-25T19:37:05.425063Z",
     "shell.execute_reply": "2025-08-25T19:37:05.424221Z",
     "shell.execute_reply.started": "2025-08-25T19:37:05.416112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel \n",
    "import torch\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Phi-4\",                         # Use this to load the base Phi-4 model from Unsloth\n",
    "    # model_name = \"/kaggle/input/phi-4-base/transformers/default/1/phi-4-base\",   # This is the same base Phi-4 model, downloaded before hand, to save time during the demo\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Version               | Precision / Quantization            | Parameter Count | Storage per Weight       | Expected Size (naive) | Actual Size (reported) | Why the Difference                                                                                     |\n",
    "| --------------------------- | ----------------------------------- | --------------- | ------------------------ | --------------------- | ---------------------- | ------------------------------------------------------------------------------------------------------ |\n",
    "| **`microsoft/phi-4`**       | BF16 (16-bit)                       | 14.7B           | 2 bytes                  | \\~29.4 GB             | \\~25 GB                | HuggingFace safetensors compress + sharding; some metadata savings                                     |\n",
    "| **`unsloth/phi-4`** | 4-bit quantized (NF4 / QLoRA style) | 14.7B           | 0.5 bytes (weights only) | \\~7.35 GB             | \\~10.39 GB             | Extra storage for quantization scales/offsets, some weights left in higher precision, padding/overhead |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parameter Efficient Fine-Tuning (PEFT) with LoRA\n",
    "\n",
    "### What is PEFT?\n",
    "\n",
    "**Parameter Efficient Fine-Tuning** allows us to adapt large models while training only a small fraction of parameters:\n",
    "\n",
    "- **Traditional Fine-tuning**: Updates all 14B parameters of Phi-4\n",
    "- **PEFT**: Updates only ~0.1-1% of parameters (millions vs billions)\n",
    "- **Benefits**: Lower memory, faster training, less overfitting risk\n",
    "\n",
    "### LoRA (Low-Rank Adaptation)\n",
    "\n",
    "**LoRA** is the most popular PEFT technique that:\n",
    "- **Freezes** original model weights\n",
    "- **Adds** small trainable matrices (rank decomposition)\n",
    "- **Approximates** weight updates with low-rank matrices\n",
    "- **Maintains** model performance with far fewer parameters\n",
    "\n",
    "### LoRA Mathematics (Simplified)\n",
    "\n",
    "```\n",
    "Original: W' = W + Î”W (full update)\n",
    "LoRA: W' = W + AÃ—B (low-rank approximation)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **W**: Original frozen weights\n",
    "- **A, B**: Small trainable matrices  \n",
    "- **AÃ—B**: Approximates the full update Î”W\n",
    "\n",
    "![](./assets/lora.png)\n",
    "\n",
    "For a weight matrix W with dimensions $d = k = 4096$, here's how LoRA (with rank $r = 16$) drastically reduces the no. of trainable parameters:\n",
    "\n",
    "| Method               | Trainable Parameters Formula                      | Example (d=4096, k=4096, r=16)        | Result         |\n",
    "| -------------------- | ------------------------------------------------- | ------------------------------------- | -------------- |\n",
    "| **Full Fine-tuning** | $d \\times k$                                      | $4096 \\times 4096$                    | **16.8M**      |\n",
    "| **LoRA**             | $(d \\times r) + (r \\times k)$                     | $(4096 \\times 16) + (16 \\times 4096)$ | **0.13M**      |\n",
    "\n",
    "**Reduction Factor** = $\\dfrac{d \\times k}{(d \\times r) + (r \\times k)} = \\dfrac{16.8M}{0.13M}$ = **128Ã— fewer** \n",
    "\n",
    "\n",
    "### Our LoRA Configuration\n",
    "\n",
    "- **r=16**: Rank of adaptation matrices (higher = more expressive)\n",
    "- **alpha=16**: Scaling factor for LoRA updates\n",
    "- **Target modules**: Attention and MLP layers we'll adapt\n",
    "- **Dropout=0**: No dropout for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.036578Z",
     "iopub.status.idle": "2025-08-25T19:37:00.036835Z",
     "shell.execute_reply": "2025-08-25T19:37:00.036692Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.036682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Medical O1-Reasoning Dataset\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "The [**medical-o1-reasoning-SFT**](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) dataset contains medical questions with:\n",
    "- **Questions**: Real medical scenarios and case studies\n",
    "- **Complex_CoT**: Detailed chain-of-thought reasoning process  \n",
    "- **Response**: Final medical answers and recommendations\n",
    "\n",
    "\n",
    "![image.png](./assets/dataset.png)\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "Each example contains:\n",
    "1. **Medical Question**: Patient presentation, symptoms, history\n",
    "2. **Reasoning Process**: Step-by-step medical thinking\n",
    "3. **Final Answer**: Diagnosis, treatment, or medical advice\n",
    "\n",
    "### Why This Dataset?\n",
    "\n",
    "- ğŸ¥ **Medical Domain**: Specialized medical knowledge\n",
    "- ğŸ§  **Reasoning Focus**: Emphasizes thinking process\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "Let's load the English portion of the medical reasoning dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.038048Z",
     "iopub.status.idle": "2025-08-25T19:37:00.038324Z",
     "shell.execute_reply": "2025-08-25T19:37:00.038217Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.038204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", 'en', split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Preprocessing and Conversation Formatting\n",
    "\n",
    "### Converting to Conversation Format\n",
    "\n",
    "To train our model effectively, we need to convert the dataset into a **conversation format** that:\n",
    "- Follows **chat templates** (user/assistant pairs)\n",
    "- Includes **reasoning tokens** (`<think>` tags)\n",
    "- Maintains **medical context** and structure\n",
    "\n",
    "### The Reasoning Pattern\n",
    "\n",
    "Our target format:\n",
    "```\n",
    "User: [Medical Question]\n",
    "Assistant: <think>[Medical Reasoning]</think>\n",
    "[Final Medical Answer]\n",
    "```\n",
    "\n",
    "This teaches the model to:\n",
    "1. **Think first** before responding\n",
    "2. **Show medical reasoning** explicitly  \n",
    "3. **Provide clear answers** after deliberation\n",
    "\n",
    "### Data Transformation Process\n",
    "\n",
    "We'll transform each dataset example:\n",
    "- **Question** â†’ User message\n",
    "- **Complex_CoT** â†’ Reasoning within `<think>` tags\n",
    "- **Response** â†’ Final assistant response\n",
    "\n",
    "Let's examine our dataset structure first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.039166Z",
     "iopub.status.idle": "2025-08-25T19:37:00.039449Z",
     "shell.execute_reply": "2025-08-25T19:37:00.039307Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.039291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert our dataset to the conversation format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.040354Z",
     "iopub.status.idle": "2025-08-25T19:37:00.040627Z",
     "shell.execute_reply": "2025-08-25T19:37:00.040484Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.040474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_to_conversations(examples):\n",
    "    questions = examples[\"Question\"]\n",
    "    reasonings = examples[\"Complex_CoT\"]\n",
    "    answers = examples[\"Response\"]\n",
    "\n",
    "    conversations = []\n",
    "    for question, reasoning, answer in zip(questions, reasonings, answers):\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": f\"<think>{reasoning}</think>\\n{answer}\"}\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    return {\n",
    "        \"conversations\": conversations,\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    convert_to_conversations,\n",
    "    batched=True,\n",
    ")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Chat Templates and Tokenization\n",
    "\n",
    "### What are Chat Templates?\n",
    "\n",
    "**Chat templates** define how conversations are formatted for different models:\n",
    "- **Special tokens**: Mark user/assistant boundaries\n",
    "- **Consistent formatting**: Ensures proper model understanding\n",
    "- **Model-specific**: Each model family has its own template\n",
    "\n",
    "### Phi-4 Chat Template\n",
    "\n",
    "Phi-4 uses specific tokens:\n",
    "- `<|im_start|>user<|im_sep|>`: Start of user message\n",
    "- `<|im_start|>assistant<|im_sep|>`: Start of assistant response\n",
    "- `<|im_end|>`: End of message\n",
    "\n",
    "### Tokenization Process\n",
    "\n",
    "We'll:\n",
    "1. **Apply chat template** to format conversations\n",
    "2. **Tokenize text** into model-readable format\n",
    "3. **Add special tokens** for training\n",
    "4. **Prepare data** for the training pipeline\n",
    "\n",
    "Let's set up the chat template and tokenize our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.042082Z",
     "iopub.status.idle": "2025-08-25T19:37:00.042325Z",
     "shell.execute_reply": "2025-08-25T19:37:00.042217Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.042204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-4\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, tokenize = False, add_generation_prompt = False\n",
    "        )\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Processed Data\n",
    "\n",
    "Let's look at how our data has been transformed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.043525Z",
     "iopub.status.idle": "2025-08-25T19:37:00.043835Z",
     "shell.execute_reply": "2025-08-25T19:37:00.043662Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.043645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(dataset[\"conversations\"][5], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.044958Z",
     "iopub.status.idle": "2025-08-25T19:37:00.045195Z",
     "shell.execute_reply": "2025-08-25T19:37:00.045091Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.045078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset[\"text\"][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pre-Training Baseline Test\n",
    "\n",
    "### Testing the Original Model\n",
    "\n",
    "Before fine-tuning, let's test how the original Phi-4 model performs on medical reasoning tasks. This will give us a **baseline** to compare against after training.\n",
    "\n",
    "### Expected vs Actual Behavior\n",
    "\n",
    "**Expected (after fine-tuning)**: \n",
    "- Step-by-step reasoning in `<think>` tags\n",
    "- Medical knowledge application\n",
    "- Systematic differential diagnosis\n",
    "\n",
    "**Current (before fine-tuning)**:\n",
    "- Direct response without reasoning\n",
    "- May lack medical depth\n",
    "- No explicit thought process\n",
    "\n",
    "Let's define our test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.046535Z",
     "iopub.status.idle": "2025-08-25T19:37:00.046872Z",
     "shell.execute_reply": "2025-08-25T19:37:00.046700Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.046686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TEST_SAMPLE = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"A 50-year-old man presents with progressive decrease in visual acuity and excessive sensitivity to light over a period of 6 months. \\\n",
    "During a slit lamp examination, discrete brown deposits on the corneal epithelium are observed in both eyes. \\\n",
    "Considering his long history of schizophrenia managed with a single antipsychotic drug, which medication would likely cause these corneal deposits?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we want our fine-tuned model to produce - a detailed reasoning process followed by a clear answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.047646Z",
     "iopub.status.idle": "2025-08-25T19:37:00.047977Z",
     "shell.execute_reply": "2025-08-25T19:37:00.047809Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.047783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_REASONING = \"So, we've got this 50-year-old guy who's having trouble with his vision getting worse and he's really sensitive to light now. \\\n",
    "This has been gradually happening over six months, which is pretty significant. Then, during a slit lamp exam, they find these unusual brown spots in his cornea, kind of like deposits. Interesting.\\\n",
    "\\\n",
    "Now, heâ€™s been dealing with schizophrenia for a long time and has been on a single antipsychotic this whole time. \\\n",
    "Thatâ€™s a big clue because not all antipsychotics affect the eyes in this way. But I remember that some specific medications do have unusual side effects like this.\\\n",
    "\\\n",
    "Okay, let me think. Which antipsychotic could cause brown deposits on the cornea? It's not something very common with all of them. Oh, right! Chlorpromazine. \\\n",
    "I've read that Chlorpromazine, which is a first-generation antipsychotic, can cause these kinds of eye changes, especially with long-term use. \\\n",
    "Things like brown or golden deposits on the cornea and even sensitivity to light.\\\n",
    "\\\n",
    "Chlorpromazine is kind of the textbook example for this. So, when you're talking about eye issues like these with a history of schizophrenia and antipsychotic use, this particular drug stands out. \\\n",
    "Itâ€™s like everything he's experiencing fits neatly into the expected side effects of Chlorpromazine.\\\n",
    "\\\n",
    "Given all that, it makes sense that Chlorpromazine is probably the medication that's causing these visual symptoms. \\\n",
    "I can't think of any other antipsychotic that has quite the same eye-related effects. Yeah, Iâ€™m pretty confident this makes sense.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SAMPLE_ANSWER = \"The medication likely causing the brown corneal deposits in this patient is Chlorpromazine. \\\n",
    "Chlorpromazine, a first-generation antipsychotic, is known for its potential side effects involving the eyes, especially with long-term use. \\\n",
    "These side effects can include the development of brown or golden deposits on the cornea and increased sensitivity to light, both of which align with the symptoms and findings observed in this patient.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Pre-Training Inference\n",
    "\n",
    "Let's see how the original model responds to our medical question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.049357Z",
     "iopub.status.idle": "2025-08-25T19:37:00.049623Z",
     "shell.execute_reply": "2025-08-25T19:37:00.049520Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.049507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-4\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    TEST_SAMPLE,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 1000,\n",
    "    use_cache = True, temperature = 1.5, min_p = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.050495Z",
     "iopub.status.idle": "2025-08-25T19:37:00.050804Z",
     "shell.execute_reply": "2025-08-25T19:37:00.050645Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.050631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"<|im_start|>assistant<|im_sep|>\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Supervised Fine-Tuning (SFT) Setup\n",
    "\n",
    "### What is Supervised Fine-Tuning?\n",
    "\n",
    "**Supervised Fine-Tuning (SFT)** is a training method where:\n",
    "- Model learns from **input-output pairs**\n",
    "- **Supervised learning**: We provide the \"correct\" responses\n",
    "- **Task-specific**: Adapts model for specific domains/tasks\n",
    "- **Behavior shaping**: Teaches new response patterns\n",
    "\n",
    "### SFT vs Other Training Methods\n",
    "\n",
    "| Method | Data Type | Purpose | Use Case |\n",
    "|--------|-----------|---------|----------|\n",
    "| **Pre-training** | Raw text | General language understanding | Foundation model |\n",
    "| **SFT** | Input-output pairs | Task-specific behavior | Medical reasoning |\n",
    "| **RLHF** | Human preferences | Alignment & safety | Helpful assistant |\n",
    "\n",
    "### Our Training Configuration\n",
    "\n",
    "**Memory & Performance:**\n",
    "- `batch_size=2`: Small batches for memory efficiency\n",
    "- `gradient_accumulation_steps=4`: Effective batch size of 8\n",
    "- `adamw_8bit`: Memory-optimized optimizer\n",
    "\n",
    "**Learning Parameters:**\n",
    "- `learning_rate=2e-4`: Moderate learning rate for stability\n",
    "- `warmup_steps=5`: Gradual learning rate increase\n",
    "- `max_steps=20`: Short training for demonstration (increase for production)\n",
    "\n",
    "**Optimization:**\n",
    "- `weight_decay=0.01`: Regularization to prevent overfitting\n",
    "- `linear` scheduler: Learning rate decay\n",
    "- `packing=False`: Process sequences individually\n",
    "\n",
    "*[Image Placeholder: Training process diagram showing Input â†’ Model â†’ Output â†’ Loss â†’ Backpropagation]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.051853Z",
     "iopub.status.idle": "2025-08-25T19:37:00.052377Z",
     "shell.execute_reply": "2025-08-25T19:37:00.052219Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.052203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 20,         # Currently, using this to show the training process quickly\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response-Only Training\n",
    "\n",
    "**Response-only training** is a crucial technique where:\n",
    "- **Loss calculated only on assistant responses**\n",
    "- **User messages ignored** during loss computation  \n",
    "- **Prevents overfitting** to user input patterns\n",
    "- **Focuses learning** on generating better responses\n",
    "\n",
    "This is important because:\n",
    "- We don't want the model to \"memorize\" user questions\n",
    "- We want it to learn how to respond appropriately\n",
    "- It improves generalization to new questions\n",
    "\n",
    "Let's configure response-only training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.053160Z",
     "iopub.status.idle": "2025-08-25T19:37:00.053423Z",
     "shell.execute_reply": "2025-08-25T19:37:00.053319Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.053306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|im_start|>user<|im_sep|>\",\n",
    "    response_part=\"<|im_start|>assistant<|im_sep|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Training Data Processing\n",
    "\n",
    "Let's inspect how our data looks after processing and verify the response-only training setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.054337Z",
     "iopub.status.idle": "2025-08-25T19:37:00.054532Z",
     "shell.execute_reply": "2025-08-25T19:37:00.054446Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.054438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.055779Z",
     "iopub.status.idle": "2025-08-25T19:37:00.056000Z",
     "shell.execute_reply": "2025-08-25T19:37:00.055904Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.055895Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.057014Z",
     "iopub.status.idle": "2025-08-25T19:37:00.057284Z",
     "shell.execute_reply": "2025-08-25T19:37:00.057142Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.057127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Process\n",
    "\n",
    "### Starting Fine-Tuning\n",
    "\n",
    "Now we'll begin the actual fine-tuning process. The training will:\n",
    "- **Update LoRA weights** based on medical reasoning examples\n",
    "- **Learn reasoning patterns** from the dataset\n",
    "- **Adapt to medical terminology** and concepts\n",
    "- **Develop step-by-step thinking** abilities\n",
    "\n",
    "**Training Progress Monitoring:**\n",
    "- **Loss values**: Should generally decrease over time\n",
    "- **Learning rate**: Will follow the linear schedule\n",
    "- **Memory usage**: Monitor GPU utilization\n",
    "- **Training speed**: Unsloth optimizations in action\n",
    "\n",
    "Let's start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.058228Z",
     "iopub.status.idle": "2025-08-25T19:37:00.058483Z",
     "shell.execute_reply": "2025-08-25T19:37:00.058387Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.058374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.059404Z",
     "iopub.status.idle": "2025-08-25T19:37:00.059608Z",
     "shell.execute_reply": "2025-08-25T19:37:00.059515Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.059506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Post-Training Evaluation\n",
    "\n",
    "### Testing Our Fine-Tuned Model\n",
    "\n",
    "Now let's test our fine-tuned model on the same medical question to see the improvement:\n",
    "\n",
    "**Expected Improvements:**\n",
    "- âœ… **Reasoning Process**: Should show `<think>` tags with medical reasoning\n",
    "- âœ… **Medical Knowledge**: Better application of medical concepts\n",
    "- âœ… **Systematic Approach**: Step-by-step diagnostic thinking\n",
    "- âœ… **Clear Conclusions**: Well-reasoned final answers\n",
    "\n",
    "**Comparison Points:**\n",
    "- **Pre-training**: Direct answers without reasoning\n",
    "- **Post-training**: Structured thinking â†’ clear answers\n",
    "\n",
    "Let's test the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.060436Z",
     "iopub.status.idle": "2025-08-25T19:37:00.060763Z",
     "shell.execute_reply": "2025-08-25T19:37:00.060602Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.060587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-4\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    TEST_SAMPLE,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs, streamer = text_streamer, max_new_tokens = 1000,\n",
    "    use_cache = True, temperature = 1.5, min_p = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Saving and Deployment\n",
    "\n",
    "### Saving the Fine-Tuned Model\n",
    "\n",
    "After successful training, we need to save our model for:\n",
    "- **Future use**: Load the model later without retraining\n",
    "- **Deployment**: Use in production applications\n",
    "- **Sharing**: Distribute to other researchers/developers\n",
    "- **Backup**: Preserve our training results\n",
    "\n",
    "**What Gets Saved:**\n",
    "- **Model weights**: The fine-tuned LoRA adapters\n",
    "- **Tokenizer**: With chat template configuration\n",
    "- **Configuration**: Model architecture and settings\n",
    "\n",
    "Let's save our fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.061775Z",
     "iopub.status.idle": "2025-08-25T19:37:00.062065Z",
     "shell.execute_reply": "2025-08-25T19:37:00.061956Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.061942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"phi-4-medical-reasoning\")\n",
    "tokenizer.save_pretrained(\"phi-4-medical-reasoning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.063262Z",
     "iopub.status.idle": "2025-08-25T19:37:00.063509Z",
     "shell.execute_reply": "2025-08-25T19:37:00.063400Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.063387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r phi-4-medical-reasoning.zip phi-4-medical-reasoning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-25T19:37:00.064438Z",
     "iopub.status.idle": "2025-08-25T19:37:00.064693Z",
     "shell.execute_reply": "2025-08-25T19:37:00.064583Z",
     "shell.execute_reply.started": "2025-08-25T19:37:00.064570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!unzip phi-4-medical-reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Save to Hugging Face Hub\n",
    "\n",
    "You need to be logged in to push to HuggingFace. \n",
    "\n",
    "Uncomment the below lines to login and push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# Push the LoRA model and tokenizer to HuggingFace Hub\n",
    "model.push_to_hub(\n",
    "    \"tezansahu/phi-4-medical-reasoning\",  # Replace \"tezansahu\" with your HuggingFace username\n",
    "    token=True,\n",
    "    private=False,  # Set to True if you want a private repository\n",
    "    commit_message=\"Fine-tuned Phi-4 for medical reasoning with LoRA\"\n",
    ")\n",
    "\n",
    "tokenizer.push_to_hub(\n",
    "    \"tezansahu/phi-4-medical-reasoning\",  # Replace \"tezansahu\" with your HuggingFace username\n",
    "    token=True,\n",
    "    private=False,\n",
    "    commit_message=\"Tokenizer for Phi-4 medical reasoning model\"\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer successfully pushed to HuggingFace Hub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the (Saved) Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:43:25.405545Z",
     "iopub.status.busy": "2025-08-25T19:43:25.405007Z",
     "iopub.status.idle": "2025-08-25T19:44:25.965255Z",
     "shell.execute_reply": "2025-08-25T19:44:25.964442Z",
     "shell.execute_reply.started": "2025-08-25T19:43:25.405512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"tezansahu/phi-4-medical-reasoning\",  # This is the model finetuned on the dataset for 1 epoch\n",
    "    model_name = \"/kaggle/input/phi-4-medical-reasoning/transformers/default/1/phi-4-medical-reasoning\", # Load from the unzipped directory\n",
    "    max_seq_length = 2048, # Use the same max_seq_length as during training\n",
    "    load_in_4bit = True,   # Use the same quantization as during training\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Interactive Medical Reasoning Interface\n",
    "\n",
    "### Building a User-Friendly Interface\n",
    "\n",
    "To make our medical reasoning model accessible, we'll create an interactive interface using **Gradio**:\n",
    "\n",
    "**Interface Features:**\n",
    "- ğŸ©º **Medical Question Input**: Text box for medical queries\n",
    "- ğŸ§  **Reasoning Display**: Shows the model's thought process\n",
    "- ğŸ’¡ **Final Answer**: Clear medical conclusions\n",
    "- âš¡ **Streaming Response**: Real-time response generation\n",
    "\n",
    "### Streaming Inference\n",
    "\n",
    "Our interface implements **streaming inference** to:\n",
    "- **Show thinking in real-time**: Users see the reasoning as it develops\n",
    "- **Improve user experience**: No waiting for complete responses\n",
    "- **Handle long reasoning**: Medical cases can have extensive thought processes\n",
    "- **Interactive feel**: More engaging than batch processing\n",
    "\n",
    "### Medical Reasoning UI Components\n",
    "\n",
    "1. **Thinking Accordion**: Collapsible section showing `<think>` content\n",
    "2. **Answer Box**: Final medical advice and conclusions  \n",
    "3. **Streaming Logic**: Separates reasoning from final answers\n",
    "4. **Medical Context**: Optimized for medical question formats\n",
    "\n",
    "\n",
    "### Sample Questions to Ask\n",
    "\n",
    "- What drug is known to reduce alcohol cravings and decrease the likelihood of resumed heavy drinking in alcoholics after completing a detoxification program?\n",
    "\n",
    "- A 21-year-old college student faints while giving a presentation in class. Friends say he looked pale and sweaty before collapsing. He regained consciousness within a minute and felt weak but alert. He denies chest pain but recalls skipping breakfast that morning. He drinks a lot of energy drinks during exams and recently started going to the gym. His ECG shows a fast irregular heartbeat. What is the most likely diagnosis?\n",
    "\n",
    "- A 29-year-old startup founder had a bad flu two weeks ago. Now he has sharp chest pain that worsens when lying flat or taking deep breaths, but eases when sitting up and leaning forward. He gets worried it might be a heart attack. On exam, thereâ€™s a scratchy sound over the chest. ECG shows diffuse ST elevation and PR depression. What is the most likely diagnosis? What is the first-line treatment?\n",
    "\n",
    "- A 52-year-old IT manager comes for a routine check-up. He feels fine but says he often gets mild headaches after long workdays and sometimes feels â€œpressureâ€ in his head when climbing stairs. He has a BMI 31. On examination, his blood pressure is 168/102 mmHg. Fundoscopy shows early retinal changes, but his kidney function is normal. What is the most likely diagnosis?\n",
    "\n",
    "\n",
    "### Creating the Interactive Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:47:03.165006Z",
     "iopub.status.busy": "2025-08-25T19:47:03.164249Z",
     "iopub.status.idle": "2025-08-25T19:47:04.143680Z",
     "shell.execute_reply": "2025-08-25T19:47:04.143144Z",
     "shell.execute_reply.started": "2025-08-25T19:47:03.164981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import TextIteratorStreamer\n",
    "import threading\n",
    "\n",
    "def stream_inference(prompt, max_new_tokens=1024):\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "    generation_kwargs = dict(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=1.2,\n",
    "        streamer=streamer,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    thinking_mode = False\n",
    "    thinking_text = \"\"\n",
    "    answer_text = \"\"\n",
    "\n",
    "    for new_text in streamer:\n",
    "        if \"<think>\" in new_text:\n",
    "            thinking_mode = True\n",
    "            new_text = new_text.replace(\"<think>\", \"\")\n",
    "        if \"</think>\" in new_text:\n",
    "            thinking_mode = False\n",
    "            new_text = new_text.replace(\"</think>\", \"\")\n",
    "            # collapse accordion by closing the thought stream\n",
    "            yield gr.update(value=thinking_text, visible=True), gr.update(value=answer_text)\n",
    "            continue\n",
    "\n",
    "        if thinking_mode:\n",
    "            thinking_text += new_text\n",
    "            yield gr.update(value=thinking_text, visible=True), gr.update(value=answer_text)\n",
    "        else:\n",
    "            answer_text += new_text\n",
    "            yield gr.update(value=thinking_text, visible=True), gr.update(value=answer_text)\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ğŸ§  Fine-Tuned Phi-4 Model for Medical Reasoning\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            user_input = gr.Textbox(label=\"Your Question\")\n",
    "            submit = gr.Button(\"Ask\")\n",
    "        with gr.Column():\n",
    "            with gr.Accordion(\"Model is thinking...\", open=True) as acc:\n",
    "                thought_stream = gr.Textbox(\n",
    "                    label=\"Chain of Thought\", value=\"\", interactive=False, visible=True\n",
    "                )\n",
    "            answer_box = gr.Textbox(label=\"Final Answer\", value=\"\", interactive=False)\n",
    "\n",
    "    submit.click(\n",
    "        stream_inference,\n",
    "        inputs=[user_input],\n",
    "        outputs=[thought_stream, answer_box]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Conclusion and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "âœ… **Transformed a Traditional LLM**: Converted Microsoft's Phi-4 from a standard response model to a reasoning-capable medical assistant\n",
    "\n",
    "âœ… **Implemented Advanced Techniques**:\n",
    "- **4-bit quantization** for memory efficiency\n",
    "- **LoRA** for parameter-efficient fine-tuning\n",
    "- **Response-only training** for focused learning\n",
    "- **Chat template optimization** for proper formatting\n",
    "\n",
    "âœ… **Medical Reasoning Enhancement**:\n",
    "- Taught the model to use `<think>` tags for reasoning\n",
    "- Trained on high-quality medical reasoning data\n",
    "- Created step-by-step diagnostic thinking patterns\n",
    "\n",
    "âœ… **Built Interactive Interface**: Created a user-friendly medical reasoning assistant with streaming responses\n",
    "\n",
    "### Key Technical Insights\n",
    "\n",
    "1. **SLMs can achieve specialized performance** comparable to larger models when fine-tuned properly\n",
    "2. **LoRA enables efficient fine-tuning** with minimal computational resources\n",
    "3. **Reasoning patterns can be learned** through structured training data\n",
    "4. **Response-only training** improves generalization significantly\n",
    "5. **Quantization** doesn't significantly impact reasoning quality\n",
    "\n",
    "### Performance Improvements\n",
    "\n",
    "**Before Fine-tuning:**\n",
    "- Direct answers without reasoning\n",
    "- Limited medical knowledge application\n",
    "- No systematic approach\n",
    "\n",
    "**After Fine-tuning:**\n",
    "- Structured medical reasoning\n",
    "- Step-by-step diagnostic thinking\n",
    "- Clear, evidence-based conclusions\n",
    "\n",
    "### Next Steps and Extensions\n",
    "\n",
    "#### ğŸ”¬ **Research Directions**\n",
    "- **Multi-modal integration**: Add medical images and charts\n",
    "- **Longer reasoning chains**: Support complex multi-step diagnoses\n",
    "- **Uncertainty quantification**: Express confidence in medical conclusions\n",
    "- **Differential diagnosis**: Generate multiple potential diagnoses\n",
    "\n",
    "#### ğŸ¥ **Clinical Applications**\n",
    "- **Medical education**: Training tool for medical students\n",
    "- **Clinical decision support**: Assist healthcare professionals\n",
    "- **Patient education**: Explain medical concepts clearly\n",
    "- **Telemedicine integration**: Support remote consultations\n",
    "\n",
    "#### âš™ï¸ **Technical Improvements**\n",
    "- **Larger training datasets**: More diverse medical cases\n",
    "- **Longer training**: More epochs for better performance\n",
    "- **Evaluation metrics**: Systematic assessment of medical reasoning quality\n",
    "- **Safety measures**: Add medical disclaimers and safety checks\n",
    "\n",
    "\n",
    "### Important Disclaimers\n",
    "\n",
    "âš ï¸ **Medical Disclaimer**: This model is for educational and research purposes only. It should never replace professional medical advice, diagnosis, or treatment.\n",
    "\n",
    "âš ï¸ **Validation Required**: All medical AI systems require extensive clinical validation before real-world use.\n",
    "\n",
    "âš ï¸ **Continuous Learning**: Medical knowledge evolves rapidly; models need regular updates.\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "**Technical Resources:**\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ‰ Congratulations!\n",
    "\n",
    "You've successfully created a reasoning-capable medical AI assistant from a traditional language model. This represents the cutting edge of AI development - teaching models to think before they respond, just like human experts do in complex domains like medicine."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 435742,
     "modelInstanceId": 418079,
     "sourceId": 541745,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
